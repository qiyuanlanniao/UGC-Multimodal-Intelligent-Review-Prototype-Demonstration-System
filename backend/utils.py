# backend/utils.py
"""
é€šç”¨å·¥å…·å‡½æ•°ï¼ˆé˜²å¼¹ç‰ˆï¼‰
"""
import torch
import numpy as np
import re
from typing import Dict, Optional


def sanitize_text(text: str) -> str:
    """
    å¼ºåŠ›æ¸…ç†æ–‡æœ¬ï¼Œåªä¿ç•™ä¸­æ–‡å­—ç¬¦ã€è‹±æ–‡å­—æ¯ã€æ•°å­—å’Œç©ºæ ¼ã€‚
    """
    if not text:
        return ""
    sanitized = re.sub(r'[^\u4e00-\u9fa5a-zA-Z0-9]', ' ', text)
    sanitized = re.sub(r'\s+', ' ', sanitized).strip()
    return sanitized


def extract_cross_modal_features(
        text: str = None,
        image_path: str = None,
        clip_model=None,
        clip_processor=None
) -> Dict:
    """
    å®‰å…¨æå–è·¨æ¨¡æ€è”åˆè¡¨å¾ç‰¹å¾ã€‚
    åŒ…å«æœ€ç»ˆä¿®å¤ï¼šç”¨try-exceptéš”ç¦»æ¨¡åž‹è°ƒç”¨ï¼Œå¤„ç†æ¨¡åž‹å¯¹ç‰¹å®šæ–‡æœ¬çš„å†…éƒ¨å´©æºƒã€‚
    """
    features = {
        "text_embed": None,
        "image_embed": None,
        "alignment_score": 0.0,
        "semantic_gap": 0.0
    }

    if not clip_model or not clip_processor:
        print("âš ï¸ æ¨¡åž‹æœªåŠ è½½ï¼Œè·³è¿‡ç‰¹å¾æå–")
        return features

    # 1. æå–æ–‡æœ¬ç‰¹å¾
    if text:
        clean_text = sanitize_text(text)
        print(f"ðŸ§¼ æ–‡æœ¬æ¸…æ´—: '{text[:40].strip()}' -> '{clean_text[:40]}'")

        if len(clean_text) > 1:
            try:
                text_inputs = clip_processor(
                    text=clean_text[:512], return_tensors="pt", padding=True
                )

                # --- é˜²å¼¹ä¿®å¤ï¼šå°†æ¨¡åž‹è°ƒç”¨æœ¬èº«éš”ç¦»åœ¨try-exceptå—ä¸­ ---
                # è¿™æ˜¯å› ä¸ºå³ä½¿tokenizerè¾“å‡ºæœ‰æ•ˆï¼Œæ¨¡åž‹å†…éƒ¨ä¹Ÿå¯èƒ½å› è¯æ±‡è¡¨é—®é¢˜è€Œå¤±è´¥ã€‚
                try:
                    with torch.no_grad():
                        text_embed = clip_model.get_text_features(**text_inputs)
                        if text_embed is not None:
                            features['text_embed'] = torch.nn.functional.normalize(text_embed, dim=-1)
                            print(f"âœ… æ–‡æœ¬ç‰¹å¾æå–æˆåŠŸ")
                        else:
                            print("âŒ æ¨¡åž‹è¿”å›žäº†Noneï¼Œå³ä½¿è¾“å…¥çœ‹èµ·æ¥æœ‰æ•ˆ")
                except Exception as model_error:
                    print(f"âŒ æ¨¡åž‹åœ¨å¤„ç†æ–‡æœ¬ '{clean_text[:40]}' æ—¶å†…éƒ¨å´©æºƒ: {model_error}")
                    # å´©æºƒåŽï¼Œtext_embed ä¿æŒä¸º Noneï¼Œæµç¨‹å¯ä»¥å®‰å…¨ç»§ç»­

            except Exception as e:
                print(f"âŒ æ–‡æœ¬ç‰¹å¾æå–çš„é¢„å¤„ç†æˆ–tokenizeæ­¥éª¤å¤±è´¥: {e}")

    # 2. æå–å›¾åƒç‰¹å¾ (æ­¤éƒ¨åˆ†é€»è¾‘ä¸å˜)
    if image_path:
        try:
            from PIL import Image
            image = Image.open(image_path).convert("RGB")
            image_inputs = clip_processor(
                images=image, return_tensors="pt"
            )
            with torch.no_grad():
                image_embed = clip_model.get_image_features(**image_inputs)
                if image_embed is not None:
                    features['image_embed'] = torch.nn.functional.normalize(image_embed, dim=-1)
        except Exception as e:
            print(f"âŒ å›¾åƒç‰¹å¾æå–å¤±è´¥: {e}")

    # 3. è®¡ç®—å¯¹é½åˆ†æ•° (æ­¤éƒ¨åˆ†é€»è¾‘ä¸å˜)
    if features['text_embed'] is not None and features['image_embed'] is not None:
        similarity = torch.cosine_similarity(features['text_embed'], features['image_embed'], dim=-1)
        features['alignment_score'] = similarity.item()
        features['semantic_gap'] = 1 - features['alignment_score']
        print(f"âœ… å›¾æ–‡å¯¹é½åˆ†æ•°: {features['alignment_score']:.3f}")

    return features


def mock_moderation_result(content_type: str = "text") -> Dict:
    """ç”Ÿæˆæ¨¡æ‹Ÿå®¡æ ¸ç»“æžœ"""
    import random
    if content_type == "text":
        return {"violation": random.random() > 0.7, "type": random.choice(['æš´åŠ›', 'è‰²æƒ…', 'æ”¿æ²»', 'è¯ˆéª—', 'æ­£å¸¸']),
                "confidence": round(random.uniform(0.7, 0.95), 3),
                "features": {"text_embed": None, "alignment_score": 0.0}, "is_mock": True}
    elif content_type == "image":
        return {"violation": random.random() > 0.7, "type": random.choice(['è‰²æƒ…', 'æš´åŠ›', 'æ­£å¸¸', 'æ­¦å™¨', 'è¡€è…¥']),
                "confidence": round(random.uniform(0.6, 0.9), 3), "ocr_text": "æ¨¡æ‹ŸOCRç»“æžœ",
                "features": {"image_embed": None, "alignment_score": 0.0}, "is_mock": True}