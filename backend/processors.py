# backend/processors.py
"""
å†…å®¹å®¡æ ¸å¤„ç†å™¨ - ç”Ÿäº§çº§å®ç° (PaddleOCR
"""
import os
import tempfile
import random
from pathlib import Path
from typing import Dict, List
import cv2
import librosa
import numpy as np
import torch

from web_ocr import browser_manager

from config import VIOLATION_KEYWORDS, CROSS_MODAL_CONFIG, VIDEO_CONFIG, SEMANTIC_VIOLATION_LABELS
from utils import extract_cross_modal_features, mock_moderation_result
from models import model_manager
import subprocess
import shutil


class TextProcessor:
    """æ–‡æœ¬å®¡æ ¸å¤„ç†å™¨ - [V3] å…³é”®è¯ + ä¼˜åŒ–ç‰ˆCLIPè¯­ä¹‰åŒ¹é… + å¥å£®é€»è¾‘"""

    @staticmethod
    def process(text: str) -> Dict:
        if not text or len(text.strip()) < 2:
            return {"violation": False, "type": "æ­£å¸¸", "confidence": 0.0, "method": "ç©ºæ–‡æœ¬"}

        # --- é˜¶æ®µä¸€: å…³é”®è¯åŒ¹é… (æœ€é«˜ä¼˜å…ˆçº§) ---
        text_lower = text.lower()
        for vtype, keywords in VIOLATION_KEYWORDS.items():
            for keyword in keywords:
                if keyword in text_lower:
                    confidence = min(0.85 + text_lower.count(keyword) * 0.1, 0.98)
                    result = {
                        "violation": True, "type": vtype, "confidence": round(confidence, 3),
                        "matched_keyword": keyword, "method": "å…³é”®è¯åŒ¹é…"
                    }
                    print(f"âœ… å…³é”®è¯å‘½ä¸­: '{keyword}' -> {vtype}")
                    # å…³é”®è¯å‘½ä¸­åï¼Œä¾ç„¶æå–ç‰¹å¾å¹¶è¿”å›
                    models = model_manager.load_models()
                    result['features'] = extract_cross_modal_features(text=text, clip_model=models.get('clip_model'),
                                                                      clip_processor=models.get('clip_processor'))
                    return result

        # --- é˜¶æ®µäºŒ: CLIP è¯­ä¹‰ç›¸ä¼¼åº¦åŒ¹é… (è‹¥å…³é”®è¯æœªå‘½ä¸­) ---
        print("ğŸ” å…³é”®è¯æœªå‘½ä¸­ï¼Œå¯åŠ¨CLIPè¯­ä¹‰ç›¸ä¼¼åº¦åˆ†æ...")
        models = model_manager.load_models()
        clip_model = models.get('clip_model')
        clip_processor = models.get('clip_processor')

        features = extract_cross_modal_features(text=text, clip_model=clip_model, clip_processor=clip_processor)

        if clip_model and features['text_embed'] is not None:
            try:
                labels = list(SEMANTIC_VIOLATION_LABELS.values())
                label_types = list(SEMANTIC_VIOLATION_LABELS.keys())

                inputs = clip_processor(text=labels, return_tensors="pt", padding=True)
                with torch.no_grad():
                    label_embeds = clip_model.get_text_features(**inputs)
                    label_embeds = torch.nn.functional.normalize(label_embeds, dim=-1)
                    probs = torch.cosine_similarity(features['text_embed'], label_embeds)

                max_prob, max_idx = probs.max(dim=0)
                confidence = max_prob.item()

                semantic_threshold = 0.26  # å¯ä»¥å¾®è°ƒè¿™ä¸ªé˜ˆå€¼

                if confidence > semantic_threshold:
                    matched_type = label_types[max_idx.item()]
                    scaled_confidence = (confidence - semantic_threshold) / (1 - semantic_threshold)

                    result = {
                        "violation": True,
                        "type": matched_type,
                        "confidence": round(min(scaled_confidence * 0.8, 0.9), 3),
                        "method": "CLIPè¯­ä¹‰åŒ¹é…",
                        "semantic_score": round(confidence, 3)
                    }
                    print(f"âœ… CLIPè¯­ä¹‰å‘½ä¸­: '{text[:30]}...' æœ€åŒ¹é… -> {matched_type} (ç›¸ä¼¼åº¦: {confidence:.3f})")
                    result['features'] = features
                    return result

            except Exception as e:
                print(f"âš ï¸ CLIPè¯­ä¹‰åˆ†æå¤±è´¥: {e}")

        # --- é˜¶æ®µä¸‰: æœ€ç»ˆåˆ¤å®š (è‹¥ä»¥ä¸Šå…¨æœªå‘½ä¸­) ---
        print("âœ… æ‰€æœ‰æ£€æŸ¥é€šè¿‡ï¼Œå†…å®¹åˆ¤å®šä¸ºå®‰å…¨ã€‚")
        result = {"violation": False, "type": "æ­£å¸¸", "confidence": 0.95, "method": "å®‰å…¨æ–‡æœ¬"}
        result['features'] = features
        return result


class ImageProcessor:
    """å›¾åƒå®¡æ ¸å¤„ç†å™¨ - ä½¿ç”¨é«˜ç²¾åº¦çš„Web OCRï¼ˆå·²ä¿®å¤è·¯å¾„é—®é¢˜ï¼‰"""

    @staticmethod
    def process(image_path: str) -> Dict:
        # --- æ ¸å¿ƒä¿®å¤ï¼šå°†ä¼ å…¥çš„ä»»ä½•è·¯å¾„éƒ½è½¬æ¢ä¸ºç»å¯¹è·¯å¾„ ---
        # Seleniumçš„ .send_keys éœ€è¦ä¸€ä¸ªç»å¯¹è·¯å¾„æ‰èƒ½æ­£ç¡®å®šä½å’Œä¸Šä¼ æ–‡ä»¶ã€‚
        # os.path.abspath() ä¼šå°† "upload\image.png" è¿™æ ·çš„ç›¸å¯¹è·¯å¾„
        # è½¬æ¢ä¸º "D:\your_project_folder\backend\upload\image.png" è¿™æ ·çš„ç»å¯¹è·¯å¾„ã€‚
        absolute_image_path = os.path.abspath(image_path)

        # å¢åŠ ä¸€ä¸ªå¥å£®æ€§æ£€æŸ¥ï¼Œç¡®ä¿æ–‡ä»¶ç¡®å®å­˜åœ¨
        if not os.path.exists(absolute_image_path):
            print(f"âŒ é”™è¯¯ï¼šå›¾ç‰‡æ–‡ä»¶åœ¨è½¬æ¢è·¯å¾„åæœªæ‰¾åˆ°: {absolute_image_path}")
            return {"error": f"å›¾ç‰‡æ–‡ä»¶æœªæ‰¾åˆ°: {absolute_image_path}", "method": "è·¯å¾„é”™è¯¯"}
        # --- ä¿®å¤ç»“æŸ ---

        print(f"ğŸ” ä½¿ç”¨Web OCRè¿›è¡Œé«˜ç²¾åº¦æ–‡æœ¬è¯†åˆ« (è·¯å¾„: {absolute_image_path})...")
        # å°†è½¬æ¢åçš„ç»å¯¹è·¯å¾„ä¼ é€’ç»™æµè§ˆå™¨ç®¡ç†å™¨
        ocr_text = browser_manager.recognize_text(absolute_image_path)

        if ocr_text:
            print(f"âœ… OCRæˆåŠŸ (Web): {ocr_text[:50]}...")
        else:
            print("âš ï¸ OCRæœªæ£€æµ‹åˆ°æ–‡æœ¬ (Web)")

        # â†“â†“â†“ åç»­çš„å®¡æ ¸é€»è¾‘å®Œå…¨å¤ç”¨ï¼Œæ— ä»»ä½•æ”¹åŠ¨ â†“â†“â†“

        # 2. OCRæ–‡æœ¬å®¡æ ¸ï¼ˆé«˜ä¼˜å…ˆçº§ï¼‰
        if len(ocr_text) > 2:
            ocr_detection = TextProcessor.process(ocr_text)
            if ocr_detection['violation']:
                models = model_manager.load_models()
                features = extract_cross_modal_features(text=ocr_text,
                                                        image_path=absolute_image_path,
                                                        clip_model=models.get('clip_model'),
                                                        clip_processor=models.get('clip_processor'))
                return {"violation": True, "type": f"OCR-{ocr_detection['type']}",
                        "confidence": ocr_detection['confidence'], "ocr_text": ocr_text[:100], "features": features,
                        "method": "Web OCRä¼˜å…ˆ"}

        # 3. CLIPè§†è§‰åˆ†æ
        from PIL import Image
        models = model_manager.load_models()
        clip_detection = None
        if models.get('clip_model') and models.get('clip_processor'):
            try:
                image = Image.open(absolute_image_path).convert("RGB")
                label_texts = ["è¿™å¼ å›¾ç‰‡åŒ…å«è‰²æƒ…å†…å®¹æˆ–è£¸éœ²", "è¿™å¼ å›¾ç‰‡åŒ…å«æš´åŠ›æˆ–è¡€è…¥ç”»é¢", "è¿™å¼ å›¾ç‰‡æ˜¯æ­£å¸¸çš„äººç‰©ç…§ç‰‡",
                               "è¿™å¼ å›¾ç‰‡æ˜¯æ­£å¸¸é£æ™¯æˆ–ç‰©å“", "è¿™å¼ å›¾ç‰‡åŒ…å«æ­¦å™¨æˆ–å±é™©ç‰©å“", "è¿™å¼ å›¾ç‰‡åŒ…å«è¡€è…¥æˆ–ææ€–ç”»é¢"]
                inputs = models['clip_processor'](text=label_texts, images=image, return_tensors="pt", padding=True)
                with torch.no_grad():
                    outputs = models['clip_model'](**inputs)
                    probs = outputs.logits_per_image.softmax(dim=1)[0]
                max_prob, max_idx = probs.max(dim=0)
                confidence = max_prob.item()
                violation_labels = [0, 1, 4, 5]
                is_violation = max_idx.item() in violation_labels and confidence > 0.55
                features = extract_cross_modal_features(text=ocr_text,
                                                        image_path=absolute_image_path,
                                                        clip_model=models.get('clip_model'),
                                                        clip_processor=models.get('clip_processor'))
                clip_detection = {"violation": is_violation,
                                  "type": ["è‰²æƒ…", "æš´åŠ›", "æ­£å¸¸", "æ­£å¸¸", "æ­¦å™¨", "è¡€è…¥"][max_idx.item()],
                                  "confidence": round(confidence, 3), "features": features, "method": "CLIPè§†è§‰",
                                  "è§†è§‰åŒ¹é…": label_texts[max_idx.item()][:15] + "..."}
                if is_violation:
                    print(f"âœ… CLIPè§†è§‰å‘½ä¸­: {clip_detection['type']} ({confidence:.3f})")
                else:
                    print(f"âœ… CLIPè§†è§‰æ­£å¸¸: {clip_detection['type']} ({confidence:.3f})")
            except Exception as e:
                print(f"âš ï¸ CLIPè§†è§‰åˆ†æå¤±è´¥: {e}")

        # 4. ä¼˜å…ˆè¿”å›CLIPç»“æœ
        if clip_detection:
            clip_detection['ocr_text'] = ocr_text[:100]
            return clip_detection

        # 5. æœ€ç»ˆé™çº§
        print("âš ï¸ æ‰€æœ‰æ–¹æ³•å¤±è´¥ï¼Œä½¿ç”¨æ¨¡æ‹Ÿç»“æœ")
        features = extract_cross_modal_features(text=ocr_text,image_path=absolute_image_path,
                                                clip_model=models.get('clip_model'),
                                                clip_processor=models.get('clip_processor'))
        result = mock_moderation_result("image")
        result['ocr_text'] = ocr_text[:100] if ocr_text else "æ— æ–‡æœ¬å†…å®¹"
        result['features'] = features
        result['method'] = "æ¨¡æ‹Ÿé™çº§"
        return result


class AudioProcessor:
    """éŸ³é¢‘å®¡æ ¸å¤„ç†å™¨ - Whisperè½¬å½• + æ–‡æœ¬åˆ†æ"""

    # ... (This class remains completely unchanged)
    @staticmethod
    def process(audio_path: str) -> Dict:
        transcript = ""
        models = model_manager.load_models()
        if models.get('whisper'):
            try:
                result = models['whisper'].transcribe(audio_path, language='zh')
                transcript = result.get('text', '').strip()
                if transcript:
                    print(f"âœ… Whisperè½¬å½•æˆåŠŸ: {transcript[:50]}...")
                else:
                    print("âš ï¸ Whisperæœªæ£€æµ‹åˆ°è¯­éŸ³")
            except Exception as e:
                print(f"âš ï¸ Whisperè½¬å½•å¤±è´¥: {e}")
        text_result = TextProcessor.process(transcript)
        features = {"transcript": transcript[:200] if transcript else "æ— è½¬å½•", "audio_duration": 0,
                    "speech_speed": "unknown"}
        try:
            y, sr = librosa.load(audio_path)
            features['audio_duration'] = len(y) / sr
            if len(transcript) > 0 and features['audio_duration'] > 0:
                char_per_sec = len(transcript) / features['audio_duration']
                if char_per_sec > 8:
                    features['speech_speed'] = "fast"
                elif char_per_sec > 4:
                    features['speech_speed'] = "normal"
                else:
                    features['speech_speed'] = "slow"
        except Exception as e:
            print(f"âš ï¸ éŸ³é¢‘ç‰¹å¾æå–å¤±è´¥: {e}")
        return {"violation": text_result['violation'], "type": text_result['type'],
                "confidence": text_result['confidence'], "transcript": transcript[:200], "features": features,
                "method": f"Whisper+{text_result['method']}"}


class VideoProcessor:
    """è§†é¢‘å®¡æ ¸å¤„ç†å™¨ - [å·²å¢å¼º] å¢åŠ è‡ªåŠ¨è½¬ç ï¼Œç¡®ä¿Webå…¼å®¹æ€§å’Œå¤„ç†ç¨³å®šæ€§"""

    @staticmethod
    def _transcode_to_h264(source_path: str) -> (str, bool):
        """
        å°†è§†é¢‘æ–‡ä»¶è½¬ç ä¸º H.264/AAC ç¼–ç çš„ MP4ã€‚
        è¿™æ˜¯ä¿è¯Webå’Œå¤„ç†åº“ï¼ˆå¦‚cv2ï¼‰å…¼å®¹æ€§çš„å…³é”®æ­¥éª¤ã€‚
        è¿”å› (å¤„ç†åçš„æ–‡ä»¶è·¯å¾„, æ˜¯å¦åˆ›å»ºäº†æ–°æ–‡ä»¶)
        """
        if not shutil.which('ffmpeg'):
            print("âš ï¸ ffmpeg æœªæ‰¾åˆ°ï¼Œè·³è¿‡è§†é¢‘è½¬ç ã€‚å¦‚æœå¤„ç†å¤±è´¥ï¼Œè¯·å®‰è£…ffmpegã€‚")
            return source_path, False

        # æ£€æŸ¥æ–‡ä»¶æ‰©å±•åï¼Œå¦‚æœå·²ç»æ˜¯ .mp4ï¼Œå¯ä»¥è€ƒè™‘è·³è¿‡ï¼ˆä½†ç¼–ç å¯èƒ½ä¸å…¼å®¹ï¼‰
        # ä¸ºäº†ç¨³å®šæ€§ï¼Œæˆ‘ä»¬ç»Ÿä¸€å¤„ç†æ‰€æœ‰ä¼ å…¥çš„è§†é¢‘

        target_path = tempfile.mktemp(suffix='.mp4')
        print(f"ğŸ”§ æ­£åœ¨å°†è§†é¢‘è½¬ç ä¸ºWebå…¼å®¹æ ¼å¼ (H.264/AAC)...")

        try:
            # -c:v libx264: ä½¿ç”¨ H.264 è§†é¢‘ç¼–ç å™¨
            # -c:a aac: ä½¿ç”¨ AAC éŸ³é¢‘ç¼–ç å™¨
            # -pix_fmt yuv420p: ä¿è¯åƒç´ æ ¼å¼çš„æœ€å¤§å…¼å®¹æ€§
            # -y: å¦‚æœç›®æ ‡æ–‡ä»¶å·²å­˜åœ¨åˆ™è¦†ç›–
            command = [
                'ffmpeg', '-i', source_path, '-c:v', 'libx264',
                '-c:a', 'aac', '-pix_fmt', 'yuv420p', '-y', target_path
            ]
            result = subprocess.run(
                command, capture_output=True, text=True, check=True, timeout=120
            )
            print(f"âœ… è§†é¢‘è½¬ç æˆåŠŸï¼Œæ–°æ–‡ä»¶ä½äº: {target_path}")
            return target_path, True
        except subprocess.CalledProcessError as e:
            print(f"âŒ è§†é¢‘è½¬ç å¤±è´¥: {e.stderr[:500]}...")
            print("...å°†å°è¯•ä½¿ç”¨åŸå§‹æ–‡ä»¶è¿›è¡Œå¤„ç†ã€‚")
            if os.path.exists(target_path): os.unlink(target_path)
            return source_path, False
        except Exception as e:
            print(f"âŒ è½¬ç è¿‡ç¨‹ä¸­å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")
            if os.path.exists(target_path): os.unlink(target_path)
            return source_path, False

    @staticmethod
    def process(video_path: str) -> Dict:
        path_to_process = video_path
        was_transcoded = False

        try:
            # --- æ ¸å¿ƒæ­¥éª¤ 1: è§†é¢‘è½¬ç  ---
            path_to_process, was_transcoded = VideoProcessor._transcode_to_h264(video_path)

            # --- æ ¸å¿ƒæ­¥éª¤ 2: ä½¿ç”¨è½¬ç åï¼ˆæˆ–åŸå§‹ï¼‰çš„æ–‡ä»¶è¿›è¡Œæ‰€æœ‰åç»­å¤„ç† ---
            results = {"frames": [], "audio": {}, "cross_modal_fusion": {}}
            audio_path = tempfile.mktemp(suffix='.wav')

            try:
                print("ğŸµ æå–éŸ³é¢‘...")
                # ä½¿ç”¨ path_to_process
                subprocess.run(
                    ['ffmpeg', '-i', path_to_process, '-vn', '-acodec', 'pcm_s16le', '-ar', '16000', '-ac', '1', '-y',
                     audio_path], capture_output=True, check=True, timeout=30)
                if os.path.exists(audio_path) and os.path.getsize(audio_path) > 0:
                    results['audio'] = AudioProcessor.process(audio_path)
                    print(f"âœ… éŸ³é¢‘å¤„ç†å®Œæˆ: {results['audio']['type']}")
                else:
                    raise Exception("éŸ³é¢‘æ–‡ä»¶ä¸ºç©º")
            except Exception as e:
                print(f"âš ï¸ éŸ³é¢‘æå–å¤±è´¥: {str(e)[:100]}...ï¼Œä½¿ç”¨æ¨¡æ‹Ÿç»“æœ")
                results['audio'] = {"violation": False, "type": "æ­£å¸¸", "confidence": 0.0, "transcript": "éŸ³é¢‘æå–å¤±è´¥"}
            finally:
                if os.path.exists(audio_path):
                    try:
                        os.unlink(audio_path)
                    except:
                        pass

            print("ğŸ¬ æå–å…³é”®å¸§...")
            # ä½¿ç”¨ path_to_process
            cap = cv2.VideoCapture(path_to_process)
            if not cap.isOpened(): raise Exception(f"æ— æ³•æ‰“å¼€è§†é¢‘æ–‡ä»¶: {path_to_process}")

            total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
            fps = cap.get(cv2.CAP_PROP_FPS)
            duration = total_frames / fps if fps > 0 else 1
            frame_positions = VIDEO_CONFIG["frame_positions"]
            frame_indices = [int(total_frames * pos) for pos in frame_positions]
            texts_from_frames = []

            for i, frame_idx in enumerate(frame_indices):
                try:
                    cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)
                    ret, frame = cap.read()
                    if ret:
                        print(f"  ğŸ“¸ å¤„ç†ç¬¬{i + 1}å¸§ (ä½ç½®: {frame_positions[i] * 100:.0f}%)")
                        frame_path = tempfile.mktemp(suffix='.jpg')
                        cv2.imwrite(frame_path, frame)
                        frame_result = ImageProcessor.process(frame_path)
                        results['frames'].append(
                            {"timestamp": round(frame_positions[i] * duration, 1), "result": frame_result})
                        if frame_result.get('ocr_text') and 'error' not in frame_result:
                            texts_from_frames.append(frame_result['ocr_text'])
                        if os.path.exists(frame_path):
                            try:
                                os.unlink(frame_path)
                            except:
                                pass
                    else:
                        print(f"âš ï¸ æ— æ³•è¯»å–ç¬¬{frame_idx}å¸§")
                except Exception as e:
                    print(f"âš ï¸ å¤„ç†å¸§{i}å¤±è´¥: {e}")
                    results['frames'].append({"timestamp": round(frame_positions[i] * duration, 1),
                                              "result": mock_moderation_result("image")})

            cap.release()
            print(f"âœ… å¸§å¤„ç†å®Œæˆ: {len(results['frames'])} å¸§")

            if texts_from_frames:
                combined_text = " ".join(texts_from_frames)
                context_result = text_processor.process(combined_text)
                if context_result['violation']:
                    results['cross_modal_fusion']['temporal_context'] = {"violation": True, "type": "è·¨å¸§å…³è”è¿è§„",
                                                                         "confidence": context_result['confidence']}

            print("âš–ï¸ æ‰§è¡Œå¤šæ¨¡æ€èåˆ...")
            final_result = {}
            modalities = []
            if results['audio'].get('violation'):
                modalities.append(
                    {'type': f"éŸ³é¢‘-{results['audio']['type']}", 'confidence': results['audio']['confidence'],
                     'weight': CROSS_MODAL_CONFIG['audio_weight']})

            violation_frames = [f for f in results['frames'] if
                                'error' not in f['result'] and f['result'].get('violation', False)]
            if violation_frames:
                max_conf_frame = max(violation_frames, key=lambda x: x['result']['confidence'])
                modalities.append({'type': f"å›¾åƒ-{max_conf_frame['result']['type']}",
                                   'confidence': max_conf_frame['result']['confidence'],
                                   'weight': CROSS_MODAL_CONFIG['image_weight']})

            if results['cross_modal_fusion'].get('temporal_context', {}).get('violation'):
                context = results['cross_modal_fusion']['temporal_context']
                modalities.append({'type': context['type'], 'confidence': context['confidence'], 'weight': 0.3})

            if modalities:
                total_score = sum(m['confidence'] * m['weight'] for m in modalities)
                total_weight = sum(m['weight'] for m in modalities)
                dominant_modality = max(modalities, key=lambda m: m['confidence'])
                dominant_type = dominant_modality['type'].split('-')[-1]
                final_result = {"violation": True, "type": dominant_type,
                                "confidence": min(round(total_score / total_weight, 3), 1.0)}
                print(f"âœ… èåˆç»“æœ: è¿è§„={True}, ä¸»è¦ç±»å‹='{dominant_type}', ç½®ä¿¡åº¦={final_result['confidence']}")
            else:
                valid_confs = [f['result'].get('confidence', 0) for f in results['frames'] if
                               'error' not in f['result']]
                avg_confidence = sum(valid_confs) / len(valid_confs) if valid_confs else 0.85
                final_result = {"violation": False, "type": "æ­£å¸¸", "confidence": round(avg_confidence, 3)}
                print(f"âœ… èåˆç»“æœ: è¿è§„={False}, ç½®ä¿¡åº¦={final_result['confidence']}")

            final_result['frames'] = results['frames']
            final_result['audio_transcript'] = results.get('audio', {}).get('transcript', 'æ— éŸ³é¢‘')
            final_result['method'] = "è§†é¢‘å¤šæ¨¡æ€èåˆ"
            return final_result

        except Exception as e:
            print(f"âŒ è§†é¢‘å¤„ç†å¤±è´¥: {str(e)[:100]}...")
            import traceback
            print(traceback.format_exc())
            return {"error": f"è§†é¢‘å¤„ç†å¤±è´¥: {str(e)}", "violation": True, "type": "å¤„ç†å¼‚å¸¸", "confidence": 1.0,
                    "frames": [], "method": "å¼‚å¸¸é™çº§"}
        finally:
            # --- æ ¸å¿ƒæ­¥éª¤ 3: æ¸…ç†è½¬ç åäº§ç”Ÿçš„ä¸´æ—¶æ–‡ä»¶ ---
            if was_transcoded and os.path.exists(path_to_process):
                try:
                    os.unlink(path_to_process)
                    print(f"ğŸ§¹ å·²æ¸…ç†ä¸´æ—¶è½¬ç æ–‡ä»¶: {path_to_process}")
                except OSError as e:
                    print(f"âš ï¸ æ¸…ç†ä¸´æ—¶è½¬ç æ–‡ä»¶å¤±è´¥: {e}")


# å¯¼å‡ºå¤„ç†å™¨å®ä¾‹
text_processor = TextProcessor()
image_processor = ImageProcessor()
audio_processor = AudioProcessor()
video_processor = VideoProcessor()
